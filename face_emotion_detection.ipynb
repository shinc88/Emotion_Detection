{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIG+9EMsGz5dETx6QDvDlW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinc88/Emotion_detection/blob/main/face_emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Emotion detection by using Deepface from video data."
      ],
      "metadata": {
        "id": "ZQ8mAHs9ogia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepface\n",
        "from deepface import DeepFace \n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import cv2 \n",
        "#from google.colab import drive # for google colab\n",
        "#from google.colab import files # for google colab\n",
        "#drive.mount('/content/gdrive') # for google colab"
      ],
      "metadata": {
        "id": "xO7LLIe7sQM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofNXtoYqoSyh"
      },
      "outputs": [],
      "source": [
        "# extract frame from the data (video) \n",
        "\n",
        "# data path (video - mp4) \n",
        "video = cv2.VideoCapture('data/Greta_UN.mp4')\n",
        "\n",
        "# get total frame of the video    \n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# determine distance of frame (choose 10 means, every 10th frame will be considered and saved)\n",
        "# lower number will cost longer calculate time \n",
        "step = 10 \n",
        "\n",
        "count = 0\n",
        "frame_list = [] \n",
        "\n",
        "# create loop to get every 10th frame from the video and store in a list\n",
        "for i in range(total_frames):\n",
        "    ret, frame = video.read()\n",
        "    if ret:\n",
        "        count += 1\n",
        "        if count % step == 0:\n",
        "            frame_list.append(frame)\n",
        "            \n",
        "video.release()\n",
        "\n",
        "# analysis emotion from frames\n",
        "\n",
        "df = pd.DataFrame()\n",
        "emotions_list = []\n",
        "\n",
        "for picture in frame_list:\n",
        "\n",
        "    objs = DeepFace.analyze(picture, actions = 'emotion', enforce_detection = False)\n",
        "    dominant_emotion = objs[0]['dominant_emotion']\n",
        "    emotions_list.append(dominant_emotion)\n",
        "\n",
        "# create data frame of the analysis\n",
        "\n",
        "df['Emotion'] = Counter(emotions_list).keys()\n",
        "df['Counts'] = Counter(emotions_list).values()\n",
        "df['Percentage'] = (df['Counts']/df['Counts'].sum())*100\n",
        "\n",
        "df_sorted = df[['Emotion','Percentage']].sort_values(by = 'Percentage', ascending=False)\n",
        "df_sorted.reset_index(drop = True, inplace=True)\n",
        "\n",
        "df_sorted"
      ]
    }
  ]
}